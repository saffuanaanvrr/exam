{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "7ATDstH_ag7B",
    "outputId": "bb03f6a9-68aa-4682-a0f6-fb7a7f56369c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"linear_regression.csv\")\n",
    "X=df.loc[:,'height'].values\n",
    "y=df.loc[:,'weight'].values\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)\n",
    "X_train=X_train.reshape(-1,1)\n",
    "y_train=y_train.reshape(-1,1)\n",
    "X_test=X_test.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)\n",
    "\n",
    "model=LinearRegression().fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "mse=mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "print(\"Mean Squared Error:\",round(mse,3))\n",
    "plt.scatter(X,y,label=\"Orginal Data\",alpha=0.5)\n",
    "plt.plot(X_test,y_pred,color=\"yellow\",linewidth=2,label=\"Regression Line\")\n",
    "plt.title(\"Line Regression\")\n",
    "plt.xlabel(\"Height\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "AVuuirWJYfnS",
    "outputId": "320bbf15-c188-4afb-e2d7-fe0ee6c4deee"
   },
   "outputs": [],
   "source": [
    "print(datetime.now(pytz.timezone('Asia/Kolkata')))\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"linear_regression.csv\")\n",
    "X=df[['height']]\n",
    "y=df['weight']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)\n",
    "\n",
    "R=LinearRegression()\n",
    "R.fit(X_train,y_train)\n",
    "y_pred=R.predict(X_test)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "print(\"Mean Squared Error:\",round(mse,3))\n",
    "plt.scatter(X,y,label=\"Orginal Data\",alpha=0.5)\n",
    "plt.plot(X_test,y_pred,color=\"yellow\",linewidth=2,label=\"Regression Line\")\n",
    "plt.title(\"Line Regression\")\n",
    "plt.xlabel(\"Height\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "oT1lxWxcem-L",
    "outputId": "19fb9327-2756-4ffa-c380-98dbcda71da5"
   },
   "outputs": [],
   "source": [
    "# print(datetime.now(pytz.timezone('Asia/Kolkata')))\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "img=cv2.imread(\"\"C:\\Users\\prana\\OneDrive\\Desktop\\DL Lab\\Cricket.jpg\"\")\n",
    "img_rgb=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "histeq=cv2.equalizeHist(gray)\n",
    "thress,bin=cv2.threshold(gray,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "print(f\"Threshold value is {thress}\")\n",
    "kernel=np.ones((3,3),np.uint8)\n",
    "opening = cv2.morphologyEx(bin,cv2.MORPH_OPEN,kernel,iterations=1)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(\"Orginal Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(gray,cmap='cool')\n",
    "plt.title(\"Gray Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(histeq)\n",
    "plt.title(\"Histogram Equilization\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(opening,cmap='gray')\n",
    "plt.title(\"Morphology Operation\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuQzLJU9sMYF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "x=np.array([3,0])\n",
    "y=np.array([3,0])\n",
    "model=Sequential([\n",
    "    Dense(1,input_dim=1,use_bias=False)\n",
    "    ])\n",
    "model.compile(optimizer='sgd',loss='mse')\n",
    "model.fit(x,y,epochs=75)\n",
    "l=model.predict(np.array([7]))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "69yNY8ejscib",
    "outputId": "210de189-0771-4eed-d796-ed6b7eff6ca2"
   },
   "outputs": [],
   "source": [
    "# print(datetime.now(pytz.timezone('Asia/Kolkata')))\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "x=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y=np.array([[0],[0],[0],[1]])\n",
    "\n",
    "model=Sequential([\n",
    "              Dense(16,input_dim=2,activation=\"relu\",use_bias=False),\n",
    "              Dense(1,activation=\"sigmoid\",use_bias=False)\n",
    "              ])\n",
    "model.compile(optimizer='adam',loss=\"mse\")\n",
    "model.fit(x,y,epochs=350)\n",
    "\n",
    "l=model.predict(np.array([[0,1]]))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5FoKBCVulLR",
    "outputId": "cf454bb3-e854-49f6-8568-a7522cd41ddb"
   },
   "outputs": [],
   "source": [
    "\n",
    "data=[5,8,15,26,10,18,3,12,6,14,11]\n",
    "\n",
    "import statistics as st\n",
    "import numpy as np\n",
    "\n",
    "print(\"With Outlier:\")\n",
    "print(\"\")\n",
    "print(\"Mean \\t\\t\\t:\",st.mean(data))\n",
    "print(\"Median\\t\\t \\t:\",st.median(data))\n",
    "print(\"Mode\\t\\t\\t:\",st.mode(data))\n",
    "print(\"Variance \\t\\t:\",st.variance(data))\n",
    "print(\"Standard Deviation\\t:\",st.stdev(data))\n",
    "\n",
    "outlier=[]\n",
    "\n",
    "def detect_outlier_zscore(data):\n",
    "    thres=3\n",
    "    mean=np.mean(data)\n",
    "    std=np.std(data)\n",
    "    for i in data:\n",
    "        z_score=(i-mean)/std\n",
    "        if(np.abs(z_score)>thres):\n",
    "            outlier.append(i)\n",
    "    return outlier\n",
    "out=detect_outlier_zscore(data)\n",
    "print(out)\n",
    "newdata=[ i for i in data if i not in out]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"\\t\\t\\tWith Outlier\\t \\tWithout Outlier\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Mean\\t\\t\\t\",round(st.mean(data),2),\"\\t\\t\\t\",round(st.mean(newdata),2))\n",
    "print(\"Median\\t\\t\\t\",round(st.median(data),2),\"\\t\\t\\t\",round(st.median(newdata),2))\n",
    "print(\"Mode\\t\\t\\t\",round(st.mode(data),2),\"\\t\\t\\t\",round(st.mode(newdata),2))\n",
    "print(\"Variance\\t\\t\",round(st.variance(data),2),\"\\t\\t\\t\",round(st.variance(newdata),2))\n",
    "print(\"Standard Deviation\\t\",round(st.stdev(data),2),\"\\t\\t\\t\",round(st.stdev(newdata),2))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "data=[5,8,15,26,10,18,3,12,6,14,11]\n",
    "outlier=[]\n",
    "\n",
    "def detect_outlier_iqr(data):\n",
    "    data=sorted(data)\n",
    "    q1=np.percentile(data,25)\n",
    "    q3=np.percentile(data,75)\n",
    "    iqr=q3-q1\n",
    "    lwr_bound=q1-(1.5*iqr)\n",
    "    upr_bound=q3+(1.5*iqr)\n",
    "\n",
    "    for i in data:\n",
    "        if(i<lwr_bound or i>upr_bound):\n",
    "            outlier.append(i)\n",
    "    return outlier\n",
    "sample_outlier=detect_outlier_iqr(data)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(sample_outlier)\n",
    "\n",
    "data=[15,101,18,7,13,16,11,21,5,15,10,5]\n",
    "\n",
    "tenth_percentile=np.percentile(data,10)\n",
    "ninetienth_percentile=np.percentile(data,90)\n",
    "b=np.where(data<tenth_percentile,tenth_percentile,data)\n",
    "b=np.where(b>ninetienth_percentile,ninetienth_percentile,b)\n",
    "\n",
    "print(\"10%\",tenth_percentile,\"\\n90%\",ninetienth_percentile,\"\\nNew array\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "TJW9MU4riYNZ",
    "outputId": "df77ee5e-2f76-4c65-82e2-395da364ac11"
   },
   "outputs": [],
   "source": [
    " # @title Default title text\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar10,emnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "(xtr,ytr),(xte,yte)=emnist.load_data()\n",
    "\n",
    "xtr,xte=xtr/255.0,xte/255.0\n",
    "ytr,yte=to_categorical(ytr),to_categorical(yte)\n",
    "\n",
    "model=Sequential([layers.Flatten(input_shape=(32,32,3)),\n",
    "                  layers.Dense(512,'relu'),\n",
    "                  layers.Dense(256,'relu'),\n",
    "                  layers.Dense(128,'relu'),\n",
    "                  layers.Dense(10,'softmax')\n",
    "                 ])\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy','precision'])\n",
    "history=model.fit(xtr,ytr,epochs=5,batch_size=64,validation_data=(xte,yte))\n",
    "_,acc,pre=model.evaluate(xte,yte)\n",
    "print(\"Test accuracy:\",round(acc*100,4))\n",
    "sample_img=xtr[5:6]\n",
    "pred=model.predict(sample_img)\n",
    "class_lab=['airplanes','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(sample_img[0])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.barh(class_lab,pred[0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JEm_zI3PixHB",
    "outputId": "4451cf3c-49f7-48f5-f3f6-2d726c8e59ef"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,initializers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "(xtr,ytr),(xte,yte)=cifar10.load_data()\n",
    "\n",
    "xtr,xte=xtr/255.0,xte/255.0\n",
    "ytr,yte=to_categorical(ytr),to_categorical(yte)\n",
    "\n",
    "def model_create(ini,L2=None):\n",
    "    model=Sequential([\n",
    "    layers.Flatten(input_shape=(32,32,3)),\n",
    "    layers.Dense(512,kernel_initializer=ini,kernel_regularizer=L2,activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(256,kernel_initializer=ini,kernel_regularizer=L2,activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(128,kernel_initializer=ini,kernel_regularizer=L2,activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(64,kernel_initializer=ini,kernel_regularizer=L2,activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(32,kernel_initializer=ini,kernel_regularizer=L2,activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(10,activation='softmax')])\n",
    "    return model\n",
    "x_ini=initializers.glorot_normal()\n",
    "k_ini=initializers.he_normal()\n",
    "x_model=model_create(x_ini,tf.keras.regularizers.l2(0.001))\n",
    "k_model=model_create(k_ini,tf.keras.regularizers.l1(0.001))\n",
    "x_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "k_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "x_history=x_model.fit(xtr,ytr,epochs=5,validation_data=(xte,yte))\n",
    "k_history=k_model.fit(xtr,ytr,epochs=5,validation_data=(xte,yte))\n",
    "_,acc=x_model.evaluate(xte,yte)\n",
    "print(\"Test accuracy of the xav ier initializer:\",round(acc*100,4))\n",
    "_,acc=k_model.evaluate(xte,yte)\n",
    "print(\"Test accuracy of the Kaiming initializer:\",round(acc*100,4))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x_history.history['accuracy'],label='Xavier(train)')\n",
    "plt.plot(x_history.history['val_accuracy'],label='Xavier(validation)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_history.history['accuracy'],label='Kaiming(train)')\n",
    "plt.plot(k_history.history['val_accuracy'],label='Kaiming(train)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "XfXfmQsib5vP",
    "outputId": "3c5dbd72-2435-4f1f-cb2c-1f46d4fdf61b"
   },
   "outputs": [],
   "source": [
    "x_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DksL64gtn5P-",
    "outputId": "2e28dbb5-0674-4927-8e71-a198957fb360"
   },
   "outputs": [],
   "source": [
    "  import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "(train_images,train_labels),(test_images,test_labels)=mnist.load_data()\n",
    "train_images,test_images=train_images/255.0,test_images/255.0\n",
    "train_labels,test_labels=to_categorical(train_labels),to_categorical(test_labels)\n",
    "model=Sequential([\n",
    "                 tf.keras.layers.Reshape((28,28,1)),\n",
    "                 tf.keras.layers.Conv2D(32,3,activation='relu'),\n",
    "                 tf.keras.layers.MaxPooling2D(),\n",
    "                 tf.keras.layers.Conv2D(64,3,activation='relu'),\n",
    "                 tf.keras.layers.MaxPooling2D(),\n",
    "                 tf.keras.layers.Conv2D(64,3,activation='relu'),\n",
    "                 tf.keras.layers.Flatten(),\n",
    "                 tf.keras.layers.Dense(64,activation='relu'),\n",
    "                 tf.keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_images,train_labels,epochs=5,batch_size=64,validation_split=0.2)\n",
    "_,acc=model.evaluate(test_images,test_labels)\n",
    "print(\"Test accuracy:\",round(acc*100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "id": "YRyhMHdUcNuz",
    "outputId": "47ae9e22-ba04-4664-cd0b-313b1db28c3d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train[:1500]\n",
    "y_train = y_train[:1500]\n",
    "x_test = x_test[:1500]\n",
    "y_test = y_test[:1500]\n",
    "\n",
    "x_train_froz, x_test_froz = x_train/255.0, x_test/255.0\n",
    "y_train_froz,y_test_froz = to_categorical(y_train),to_categorical(y_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(x_train_froz,y_train_froz, epochs = 5,batch_size=64,validation_split=0.2)\n",
    "\n",
    "x_train_rgb = tf.image.grayscale_to_rgb(tf.expand_dims(x_train, axis =-1))\n",
    "x_test_rgb = tf.image.grayscale_to_rgb(tf.expand_dims(x_test, axis =-1))\n",
    "\n",
    "x_train_resized = tf.image.resize(x_train_rgb,(224,224))\n",
    "x_test_resized = tf.image.resize(x_test_rgb,(224,224))\n",
    "\n",
    "x_train_resized = x_train_resized/255.0\n",
    "x_test_resized = x_test_resized/255.0\n",
    "\n",
    "y_train_rgb = to_categorical(y_train, num_classes=10)\n",
    "y_test_rgb = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "base_model = VGG19(weights='imagenet', include_top=False,input_shape=(224,224,3))\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_2 = Model(inputs=base_model.input, outputs = output)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "histo = model_2.fit(x_train_resized,y_train_rgb, epochs = 5,batch_size=64,validation_split=0.2)\n",
    "\n",
    "loss,accuracy = model_2.evaluate(x_test_resized, y_test_rgb)\n",
    "\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "plt.plot(hist.history['accuracy'],label='Fixed Feature Extractor(Train)')\n",
    "plt.plot(hist.history['val_accuracy'],label='Fixed Feature Extractor(Validate)')\n",
    "plt.plot(histo.history['accuracy'],label='Fine-Tuning(Train)')\n",
    "plt.plot(histo.history['val_accuracy'],label='Fine-Tuning(Validate)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdZXAQPWxPOz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
    "num_words=10000\n",
    "max_length=200\n",
    "(xtr,ytr),(xte,yte)=imdb.load_data(num_words=num_words)\n",
    "xtr,xte=pad_sequences(xtr,maxlen=max_length),pad_sequences(xte,maxlen=max_length)\n",
    "model=Sequential([\n",
    "    Embedding(input_dim=num_words,output_dim=128,input_length=max_length),\n",
    "    LSTM(128),\n",
    "    Dense(1,activation='sigmoid')\n",
    "    ])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(xtr,ytr,validation_split=0.2,epochs=50,batch_size=64)\n",
    "loss,acc=model.evaluate(xte,yte)\n",
    "print(\"Test accuracy:\",round(acc*100,4))\n",
    "test_seq=np.reshape(xte[1],(1,-1))\n",
    "pred=model.predict(test_seq)[0]\n",
    "if round(pred[0])==1:\n",
    "  print('Positive Review')\n",
    "else:\n",
    "  print('Negative Review')\n",
    "print(yte[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Oz_gYewrjCI",
    "outputId": "20f66fc2-d510-41b3-effd-eaa5ea4399c6"
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def preprocess_input(text):\n",
    "    words = text.lower().split()\n",
    "    sequence = [word_index.get(word, 2) for word in words if word in word_index]  # '2' is usually reserved for \"unknown\" words\n",
    "    padded_sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "    return padded_sequence\n",
    "\n",
    "user_input = \"i did not like the movie\"\n",
    "processed_input = preprocess_input(user_input)\n",
    "\n",
    "pred = model.predict(processed_input)\n",
    "print(pred)\n",
    "if round(pred[0][0]) == 1:\n",
    "    print('Positive Review')\n",
    "else:\n",
    "  print('Negative Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I4Ek0XkDdKmx",
    "outputId": "46898b6b-394e-487a-c6fd-1402ddc27ef7"
   },
   "outputs": [],
   "source": [
    "print(datetime.now(pytz.timezone('Asia/Kolkata')))\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU\n",
    "(xtr, ytr), (xte, yte) = imdb.load_data(num_words=10000)\n",
    "xtr = pad_sequences(xtr, maxlen=200)\n",
    "xte = pad_sequences(xte, maxlen=200)\n",
    "l_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128,input_length=200),\n",
    "    LSTM(units=128),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "l_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "l_history = l_model.fit(xtr, ytr, epochs=5, batch_size=64, validation_split=0.2)\n",
    "loss, acc = l_model.evaluate(xte, yte)\n",
    "print('Test Accuracy of LSTM:', round(acc * 100, 2))\n",
    "g_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128,input_length=200),\n",
    "    GRU(units=128),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "g_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "g_history = g_model.fit(xtr, ytr, epochs=5, batch_size=64, validation_split=0.2)\n",
    "loss, acc = g_model.evaluate(xte, yte)\n",
    "print('Test Accuracy of GRU:', round(acc * 100, 2))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(l_history.history['accuracy'],label='LSTM(Train)')\n",
    "plt.plot(l_history.history['val_accuracy'],label='LSTM(Validation)')\n",
    "plt.plot(g_history.history['accuracy'],label='GRU(Train)')\n",
    "plt.plot(g_history.history['val_accuracy'],label='GRU(Validation)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(l_history.history['loss'],label='LSTM(Train)')\n",
    "plt.plot(l_history.history['val_loss'],label='LSTM(Validation)')\n",
    "plt.plot(g_history.history['loss'],label='GRU(Train)')\n",
    "plt.plot(g_history.history['val_loss'],label='GRU(Validation)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_L-qwzXpsrSn",
    "outputId": "129d0891-8739-4225-fd24-2b780d2a3437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nihal\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nifty1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31536\\1237022458.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nifty1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'High'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Low'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'High'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Low'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nifty1.csv'"
     ]
    }
   ],
   "source": [
    "#print(datetime.now(pytz.timezone('Asia/Kolkata')))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data=pd.read_csv(\"nifty1.csv\",index_col=\"Date\",parse_dates=True)\n",
    "scaler=MinMaxScaler()\n",
    "data[['Open','High','Low','Close']]=scaler.fit_transform(data[['Open','High','Low','Close']])\n",
    "train_data,test_data=train_test_split(data,test_size=0.2,shuffle=False)\n",
    "model=Sequential([\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1,activation='linear'),\n",
    "])\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model.fit(train_data[['Open','High','Low']],train_data['Close'],epochs=100)\n",
    "predicted_closing_prices=model.predict(test_data[['Open','High','Low']])\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_data.index,test_data['Close'],label='Actual')\n",
    "plt.plot(test_data.index,predicted_closing_prices,label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.title('NIFTY-50 Stock Price Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mae=mean_absolute_error(test_data['Close'],predicted_closing_prices)\n",
    "print(\"Mean Absolute Error:\",round(mae,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
